{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.spawn import _main\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_means = pd.read_csv('/home/ubuntu/CS231N/cs231n_final_project/data_preprocessing/X_train_channel_means.csv',index_col=False).to_numpy()\n",
    "X_train_means[:,1]\n",
    "# norm_trans = T.Normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(FCNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc1 = nn.Linear(self.input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "def load_data(train =True,valid = True,test = False):\n",
    "    # Load the raw CIFAR-10 data.\n",
    "    data_dir = '/home/ubuntu/CS231N/data/split_datasets/'\n",
    "    # data_dir = \"../../data/\"\n",
    "    X_train,y_train,X_valid,y_valid,X_test,y_test = None, None, None, None, None, None\n",
    "    if train:\n",
    "        X_train = pd.read_pickle(data_dir + \"train_data.pkl\").to_numpy()\n",
    "        y_train = pd.read_pickle(data_dir + \"train_labels.pkl\").to_numpy()\n",
    "        y_train = y_train.flatten().astype(np.int64)\n",
    "        print('Training data shape: ', X_train.shape)\n",
    "        print('Training labels shape: ', y_train.shape)\n",
    "    if valid:\n",
    "        X_valid = pd.read_pickle(data_dir + \"valid_data.pkl\").to_numpy()\n",
    "        y_valid = pd.read_pickle(data_dir + \"valid_labels.pkl\").to_numpy()\n",
    "        y_valid = y_valid.flatten().astype(np.int64)\n",
    "        print('Validation data shape: ', X_valid.shape)\n",
    "        print('Validation labels shape: ', y_valid.shape)\n",
    "    if test:\n",
    "        X_test = pd.read_pickle(data_dir + \"test_data.pkl\").to_numpy()\n",
    "        y_test = pd.read_pickle(data_dir + \"test_labels.pkl\").to_numpy()\n",
    "        y_test = y_test.flatten().astype(np.int64)\n",
    "        print('Test data shape: ', X_test.shape)\n",
    "        print('Test labels shape: ', y_test.shape)\n",
    "    # As a sanity check, we print out the size of the data we output.\n",
    "    # if didn;t load the data, it is None\n",
    "    return X_train,y_train,X_valid,y_valid,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=1):    \n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "        num_correct, num_samples = check_accuracy(loader_val, model)\n",
    "        epoch_acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f) after epoch %d' % (num_correct, num_samples, 100 * epoch_acc, e+1))\n",
    "\n",
    "    return epoch_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
