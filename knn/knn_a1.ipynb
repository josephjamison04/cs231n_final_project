{"cells":[{"cell_type":"code","execution_count":14,"id":"0ff438a8","metadata":{"executionInfo":{"elapsed":1487,"status":"ok","timestamp":1682023915390,"user":{"displayName":"Umar Dizon Maniku","userId":"15878917710239696383"},"user_tz":420},"id":"0ff438a8","tags":["pdf-ignore"]},"outputs":[],"source":["# Run some setup code for this notebook.\n","import numpy as np\n","import pandas as pd\n","from sklearn.neighbors import KNeighborsClassifier\n","import torch\n","import pickle\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","# from cs231n.data_utils import load_CIFAR10\n","# import matplotlib.pyplot as plt\n","\n","# # This is a bit of magic to make matplotlib figures appear inline in the notebook\n","# # rather than in a new window.\n","# %matplotlib inline\n","# plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","# plt.rcParams['image.interpolation'] = 'nearest'\n","# plt.rcParams['image.cmap'] = 'gray'\n","\n","# # Some more magic so that the notebook will reload external python modules;\n","# # see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","# %load_ext autoreload\n","# %autoreload 2"]},{"cell_type":"code","execution_count":2,"id":"75a3fe89","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6163,"status":"ok","timestamp":1682023921548,"user":{"displayName":"Umar Dizon Maniku","userId":"15878917710239696383"},"user_tz":420},"id":"75a3fe89","outputId":"26f23715-310c-40f0-9f30-530badad2eb2","tags":["pdf-ignore"]},"outputs":[{"name":"stdout","output_type":"stream","text":["Training data shape:  (64000, 49152)\n","Training labels shape:  (64000,)\n","Test data shape:  (20000, 49152)\n","Test labels shape:  (20000,)\n"]}],"source":["# Load the raw CIFAR-10 data.\n","# data_dir = '/home/ubuntu/CS231N/data/split_datasets/'\n","data_dir = \"../../data/split-datasets/\"\n","\n","X_train = pd.read_pickle(data_dir + \"train_data.pkl\").to_numpy()\n","y_train = pd.read_pickle(data_dir + \"train_labels.pkl\").to_numpy()\n","X_valid = pd.read_pickle(data_dir + \"valid_data.pkl\").to_numpy()\n","y_valid = pd.read_pickle(data_dir + \"valid_labels.pkl\").to_numpy()\n","X_test = pd.read_pickle(data_dir + \"test_data.pkl\").to_numpy()\n","y_test = pd.read_pickle(data_dir + \"test_labels.pkl\").to_numpy()\n","\n","y_train = y_train.flatten().astype(np.int64)\n","y_valid = y_valid.flatten().astype(np.int64)\n","y_test = y_test.flatten().astype(np.int64)\n","\n","# As a sanity check, we print out the size of the training and test data.\n","print('Training data shape: ', X_train.shape)\n","print('Training labels shape: ', y_train.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)"]},{"cell_type":"code","execution_count":3,"id":"ec9b9932","metadata":{},"outputs":[],"source":["X_train = X_train.reshape(-1, 3, 128, 128)"]},{"cell_type":"code","execution_count":2,"id":"3e5a17af","metadata":{},"outputs":[],"source":["channel_means = pd.read_csv(\"../data_preprocessing/X_train_channel_means.csv\", index_col=0)\n","channel_sds = pd.read_csv(\"../data_preprocessing/X_train_channel_stds.csv\", index_col=0)"]},{"cell_type":"code","execution_count":3,"id":"661b88c3","metadata":{},"outputs":[],"source":["class YourDataset(Dataset):\n","\n","    def __init__(self, X_Train, Y_Train, transform=None):\n","        self.X_Train = X_Train\n","        self.Y_Train = Y_Train\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.X_Train)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        x = self.X_Train[idx]\n","        y = self.Y_Train[idx]\n","\n","        if self.transform:\n","            x = self.transform(x)\n","            y = self.transform(y)\n","\n","        return x, y"]},{"cell_type":"code","execution_count":34,"id":"eee80fc2","metadata":{},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, data, targets, transform=None):\n","        self.data = data\n","        self.targets = torch.LongTensor(targets)\n","        self.transform = transform\n","        \n","    def __getitem__(self, index):\n","        x = self.data[index]\n","        y = self.targets[index]\n","        \n","        if self.transform:\n","            x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1,2,0))\n","            x = self.transform(x)\n","        \n","        return x, y\n","    \n","    def __len__(self):\n","        return len(self.data)\n","\n","# Let's create 10 RGB images of size 128x128 and 10 labels {0, 1}\n","# data = list(np.random.randint(0, 255, size=(10, 3, 128, 128)))\n","# targets = list(np.random.randint(2, size=(10)))\n","\n","data_dir = \"../../data/split-datasets/\"\n","file = open(data_dir + 'train_data.pkl', 'rb')\n","X_train = pickle.load(file).to_numpy()\n","X_train = X_train.reshape(-1, 3, 128, 128)\n","file.close()\n","\n","file = open(data_dir + 'train_labels.pkl', 'rb')\n","y_train = pickle.load(file).to_numpy()\n","y_train = y_train.flatten().astype(np.int64)\n","file.close()\n","\n","transform = transforms.Compose(\n","    [\n","        transforms.Resize(128), \n","        transforms.ToTensor(),\n","        transforms.Normalize(channel_means.to_numpy().reshape(-1), channel_sds.to_numpy().reshape(-1))\n","        ])\n","dataset = MyDataset(X_train, y_train, transform=transform)\n","dataloader = DataLoader(dataset, shuffle=False, batch_size=5)"]},{"cell_type":"code","execution_count":38,"id":"bd1c51fe","metadata":{},"outputs":[],"source":["inputs, classes = next(iter(dataloader))  "]},{"cell_type":"code","execution_count":39,"id":"b07030f9","metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[0.7961, 0.8078, 0.8235,  ..., 0.5216, 0.5098, 0.4980],\n","         [0.8078, 0.8196, 0.8353,  ..., 0.5333, 0.5216, 0.5098],\n","         [0.8235, 0.8353, 0.8510,  ..., 0.5569, 0.5333, 0.5255],\n","         ...,\n","         [0.0471, 0.0588, 0.0667,  ..., 0.0745, 0.0902, 0.0863],\n","         [0.0667, 0.0706, 0.0784,  ..., 0.0667, 0.0784, 0.0745],\n","         [0.0824, 0.0863, 0.0902,  ..., 0.0588, 0.0706, 0.0667]],\n","\n","        [[0.7725, 0.7843, 0.8000,  ..., 0.4588, 0.4471, 0.4353],\n","         [0.7843, 0.7961, 0.8118,  ..., 0.4706, 0.4588, 0.4471],\n","         [0.8039, 0.8157, 0.8314,  ..., 0.4902, 0.4745, 0.4667],\n","         ...,\n","         [0.0549, 0.0667, 0.0745,  ..., 0.0824, 0.0980, 0.0941],\n","         [0.0745, 0.0784, 0.0863,  ..., 0.0745, 0.0863, 0.0824],\n","         [0.0902, 0.0941, 0.0980,  ..., 0.0667, 0.0784, 0.0745]],\n","\n","        [[0.7765, 0.7882, 0.8039,  ..., 0.4314, 0.4196, 0.4078],\n","         [0.7882, 0.8000, 0.8157,  ..., 0.4431, 0.4314, 0.4196],\n","         [0.8078, 0.8196, 0.8353,  ..., 0.4549, 0.4392, 0.4314],\n","         ...,\n","         [0.0549, 0.0667, 0.0745,  ..., 0.0863, 0.1020, 0.0980],\n","         [0.0745, 0.0784, 0.0863,  ..., 0.0784, 0.0902, 0.0863],\n","         [0.0902, 0.0941, 0.0980,  ..., 0.0706, 0.0824, 0.0784]]])"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["inputs[0]"]},{"cell_type":"code","execution_count":40,"id":"c25d7425","metadata":{},"outputs":[{"data":{"text/plain":["tensor(44)"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["classes[0]"]},{"cell_type":"code","execution_count":19,"id":"8ad5adb3","metadata":{},"outputs":[{"data":{"text/plain":["tensor([44, 37, 46, 88, 51])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["classes"]},{"cell_type":"code","execution_count":4,"id":"6ff76148","metadata":{},"outputs":[],"source":["data_dir = \"../../data/split-datasets/\"\n","file = open(data_dir + 'train_data.pkl', 'rb')\n","X_train = pickle.load(file).to_numpy()\n","# X_train = X_train.reshape(-1, 128, 128, 3)\n","file.close()"]},{"cell_type":"code","execution_count":5,"id":"5fc67912","metadata":{},"outputs":[],"source":["X_train = torch.tensor(X_train, dtype=torch.float)"]},{"cell_type":"code","execution_count":6,"id":"f1df3b6f","metadata":{},"outputs":[],"source":["file = open(data_dir + 'train_labels.pkl', 'rb')\n","y_train = pickle.load(file).to_numpy()\n","y_train = y_train.flatten().astype(np.int64)\n","file.close()"]},{"cell_type":"code","execution_count":7,"id":"67bbb507","metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([64000, 49152])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape"]},{"cell_type":"code","execution_count":10,"id":"754d0c0c","metadata":{},"outputs":[],"source":["your_dataset = YourDataset(X_train, y_train, transform=transforms.Compose(\n","    [\n","        # transforms.ToPILImage(),\n","        transforms.Resize((128, 128)),\n","        # transforms.Normalize(channel_means.values, channel_sds.values)\n","        ]\n","    ))\n","\n","your_data_loader = DataLoader(your_dataset, batch_size=1, shuffle=True, num_workers=0)"]},{"cell_type":"code","execution_count":11,"id":"6c984409","metadata":{},"outputs":[{"ename":"TypeError","evalue":"Tensor is not a torch image.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs, classes \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(your_data_loader))  \n","File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mYourDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mY_Train[idx]\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m---> 19\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(x)\n\u001b[1;32m     20\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(y)\n\u001b[1;32m     22\u001b[0m \u001b[39mreturn\u001b[39;00m x, y\n","File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n","File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n","File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torchvision/transforms/functional.py:476\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[39mif\u001b[39;00m max_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    471\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    472\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    474\u001b[0m         )\n\u001b[0;32m--> 476\u001b[0m _, image_height, image_width \u001b[39m=\u001b[39m get_dimensions(img)\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(size, \u001b[39mint\u001b[39m):\n\u001b[1;32m    478\u001b[0m     size \u001b[39m=\u001b[39m [size]\n","File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torchvision/transforms/functional.py:76\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     74\u001b[0m     _log_api_usage_once(get_dimensions)\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mget_dimensions(img)\n\u001b[1;32m     78\u001b[0m \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39mget_dimensions(img)\n","File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:19\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dimensions\u001b[39m(img: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[0;32m---> 19\u001b[0m     _assert_image_tensor(img)\n\u001b[1;32m     20\u001b[0m     channels \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m img\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m img\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m]\n\u001b[1;32m     21\u001b[0m     height, width \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\n","File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:15\u001b[0m, in \u001b[0;36m_assert_image_tensor\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_image_tensor\u001b[39m(img: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_tensor_a_torch_image(img):\n\u001b[0;32m---> 15\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTensor is not a torch image.\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[0;31mTypeError\u001b[0m: Tensor is not a torch image."]}],"source":["inputs, classes = next(iter(your_data_loader))  "]},{"cell_type":"code","execution_count":7,"id":"0ee79020","metadata":{},"outputs":[],"source":["X_train = torch.tensor(X_train, dtype=torch.float)"]},{"cell_type":"code","execution_count":8,"id":"069e26d0","metadata":{},"outputs":[],"source":["train_normalize = T.Normalize(channel_means.values, channel_sds.values)"]},{"cell_type":"code","execution_count":2,"id":"f0bc02c3","metadata":{},"outputs":[],"source":["data_dir = \"../../data/split-datasets/\"\n","\n","X_train = pd.read_pickle(data_dir + \"train_data.pkl\").to_numpy()\n","X_train = X_train.reshape(-1, 3, 128, 128)\n","X_train_0 = X_train[:, 0, : :]\n","X_train_1 = X_train[:, 1, : :]\n","X_train_2 = X_train[:, 2, : :]\n","del X_train"]},{"cell_type":"code","execution_count":null,"id":"21757f41","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"id":"a8aff94c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682023924876,"user":{"displayName":"Umar Dizon Maniku","userId":"15878917710239696383"},"user_tz":420},"id":"a8aff94c","outputId":"fc24f365-9c4b-4025-a720-d06e5e20029e","tags":["pdf-ignore"]},"outputs":[{"name":"stdout","output_type":"stream","text":["(5000, 49152) (5000, 49152) (5000, 49152)\n"]}],"source":["# Subsample the data for more efficient code execution in this exercise\n","num_training = 5000\n","mask = list(range(num_training))\n","X_train = X_train[mask]\n","y_train = y_train[mask]\n","\n","num_valid = 5000\n","mask = list(range(num_valid))\n","X_valid = X_valid[mask]\n","y_valid = y_valid[mask]\n","\n","num_test = 5000\n","mask = list(range(num_test))\n","X_test = X_test[mask]\n","y_test = y_test[mask]\n","\n","# Reshape the image data into rows\n","X_train = np.reshape(X_train, (X_train.shape[0], -1))\n","X_valid = np.reshape(X_valid, (X_valid.shape[0], -1))\n","X_test = np.reshape(X_test, (X_test.shape[0], -1))\n","print(X_train.shape, X_valid.shape, X_test.shape)"]},{"cell_type":"code","execution_count":22,"id":"8e9d50b3","metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"],"text/plain":["KNeighborsClassifier(n_neighbors=3)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["neigh = KNeighborsClassifier(n_neighbors=3)\n","neigh.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":24,"id":"4e1c0c9d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n"]}],"source":["# k_s = range(1, 11)\n","# k_scores = {}\n","# for k in k_s:\n","#     print(k)\n","#     knn = KNeighborsClassifier(n_neighbors=k)\n","#     scores = cross_val_score(knn, X_valid, y_valid, cv=5, scoring='accuracy')\n","#     k_scores[k] = scores.mean()"]},{"cell_type":"code","execution_count":25,"id":"7c8bf369","metadata":{},"outputs":[{"data":{"text/plain":["{1: 0.027200000000000002,\n"," 2: 0.0308,\n"," 3: 0.034,\n"," 4: 0.032,\n"," 5: 0.0306,\n"," 6: 0.0288,\n"," 7: 0.029199999999999997,\n"," 8: 0.0306,\n"," 9: 0.0316,\n"," 10: 0.0332}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# k_scores"]},{"cell_type":"code","execution_count":null,"id":"92a0d9bd","metadata":{},"outputs":[],"source":["# knn = KNeighborsClassifier(n_neighbors=3)\n","# neigh.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":4,"id":"e93c69f8","metadata":{},"outputs":[{"data":{"text/plain":["0.03775"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Full training set, ~55 mins\n","# neigh.score(X_test, y_test)"]},{"cell_type":"code","execution_count":5,"id":"ec5a2478","metadata":{},"outputs":[],"source":["from joblib import dump, load\n","dump(neigh, 'knn_model.joblib') "]},{"cell_type":"code","execution_count":7,"id":"5737a769","metadata":{},"outputs":[],"source":["neigh_load = load(\"knn_model.joblib\")"]},{"cell_type":"code","execution_count":null,"id":"0ff7fa97","metadata":{},"outputs":[],"source":["neigh_load.predict(X_test[0:2])"]},{"cell_type":"code","execution_count":null,"id":"21f6230d","metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.7 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":5}
